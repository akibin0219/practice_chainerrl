{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-nashville",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fancy-fruit",
   "metadata": {},
   "source": [
    "# テスト的に一度実際のbotで実装を試してみるノートブック，ごちゃごちゃしそうなので注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eastern-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\cupy\\_environment.py:399: UserWarning: \n",
      "cudnn library could not be loaded.\n",
      "\n",
      "Reason: ImportError (DLL load failed while importing cudnn: 指定されたモジュールが見つかりません。)\n",
      "\n",
      "You can install the library by:\n",
      "\n",
      "  $ python -m cupyx.tools.install_library --library cudnn --cuda 11.2\n",
      "\n",
      "  warnings.warn(msg)\n",
      "c:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\chainer\\backends\\cuda.py:154: UserWarning: cuDNN is not enabled.\n",
      "Please reinstall CuPy after you install cudnn\n",
      "(see https://docs-cupy.chainer.org/en/stable/install.html#install-cudnn).\n",
      "  warnings.warn(\n",
      "c:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\chainer\\_environment_check.py:72: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "CuPy (cupy-cuda112) version 10.6.0 may not be compatible with this version of Chainer.\n",
      "Please consider installing the supported version by running:\n",
      "  $ pip install 'cupy-cuda112>=7.7.0,<8.0.0'\n",
      "\n",
      "See the following page for more details:\n",
      "  https://docs.cupy.dev/en/latest/install.html\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(msg.format(\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore',  category=pd.errors.PerformanceWarning)#解決できない警告は無視"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-basket",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "japanese-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# place_name='asiya'\n",
    "\n",
    "# result_filepath=\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "# sample_race_df=pd.read_csv(result_filepath)\n",
    "# sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "# sample_race_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-century",
   "metadata": {},
   "source": [
    "# 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surprising-shaft",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def trans_date_type(df):\n",
    "    df['date']=pd.to_datetime(df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    #df['year']=df['date'].dt.year\n",
    "    #df=df.drop('date',axis=1)\n",
    "    return df\n",
    "\n",
    "def data_making(df):#クラスタリングなし、ボート、艇番号無し\n",
    "    warnings.simplefilter('ignore',  category=pd.errors.PerformanceWarning)#解決できない警告は無視\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22})#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02})#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02})\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col])#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    clustar_target_df=result_df_dummie\n",
    "    clustaring_df=clustar_target_df\n",
    "    model_df=clustaring_df\n",
    "    model_df=trans_date_type(model_df)\n",
    "    return model_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-bridal",
   "metadata": {},
   "source": [
    "# boatraceでの環境の定義（cartpoleを参考に）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-acquisition",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 出力を1～28の確率分布とし，それを係数として購買を行う＝確率分布で出力するやり方がわからない，最もらしい行動を一つピックアップされてしまう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "painted-dialogue",
   "metadata": {
    "code_folding": [
     28,
     41,
     107
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BoatraceEnv:\n",
    "#class BoatraceEnv(必要であればここに購買金額の係数等を書いてあげる):\n",
    "    def __init__(self,race_df,cash,bet_coefficient=10000,end_th_cash_magni=0.5):#メンバ等の定義\n",
    "        \n",
    "        self.st_cash = cash#初めに持っている所持金\n",
    "        self.cash = cash#所持金（初めは上のものと同じ値が入るが，こちらの変数はゲームが進んでいくと更新されていく）\n",
    "        self.end_th_cash = cash*end_th_cash_magni#所持金がいくらを下回ったらゲームオーバーか決めるための閾値金額(end_th_cashで何パーセントを下回ったらアウトかを指定)\n",
    "        self.bet_coefficient = bet_coefficient#購買時を想定して定数倍する係数\n",
    "        self.random_state = 7#何かと生成をするときに使うためのシード値\n",
    "        self.race_df=race_df#全部のレース情報のまとめdfを格納\n",
    "        \n",
    "        #self.race_ID=0#どこのレースを抜き出すかどうかの番号付け\n",
    "        self.race_ID=0#どこのレースを抜き出すかどうかの番号付け(インデックス参照で抜き出す)\n",
    "        self.total_gain = None#現時点と開始時を比較した時の利益率(reset()で要リセット)\n",
    "        self.total_use = 0#現時点までの総使用金額(reset()で要リセット)\n",
    "        self.total_get = 0#現時点までの総獲得金額(reset()で要リセット)\n",
    "        self.state = None#1レースあたりのデータ格納用のメンバ\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "        #Q関数定義時関連の処理\n",
    "        self.n_action=28#行動の種類(28通りのcomを購買対象とする)\n",
    "        sample_df=self.race_df.copy()\n",
    "        sample_df=sample_df.drop(['date',\"result_com\",\"money\"],axis=1)\n",
    "        self.obs_size=len(sample_df.columns)#予測に使う情報の次元数\n",
    "\n",
    "#     def seed(self, seed=None):\n",
    "#         self.np_random, seed = seeding.np_random(seed)\n",
    "#         return [seed]\n",
    "    def split_race_data(self,race_ID):#レースのID(インデックス)を渡して，レースの情報を学習時などに使いやすい形に切り分けるメソッド\n",
    "        race_row=self.race_df.iloc[race_ID]#対象のレースを切り抜き(series型)\n",
    "        result_com=race_row['result_com']\n",
    "        return_money=race_row['money']\n",
    "        racer_ID=race_row['money']\n",
    "        race_date=race_row['date']\n",
    "        race_data=race_row.drop(['date',\"result_com\",\"money\"]).values\n",
    "        race_data_label=race_row.drop(['date',\"result_com\",\"money\"]).index\n",
    "                                \n",
    "        #race_data:選手情報やボート・モータ番号などの予測用データが入った配列\n",
    "        #race_data_label:予測用データの役割を示す配列(pandasの列名（カラム）がこれに該当する)\n",
    "        return result_com,return_money,race_data,race_data_label,race_date\n",
    "\n",
    "    def step(self, action):#ステップ（ゲームの報酬計算と次の環境の取得）\n",
    "        race_ID=self.race_ID\n",
    "        result_com,return_money,race_data,race_data_label,race_date=self.split_race_data(self.race_ID)#いろいろ算出用にレースデータの抜き取り\n",
    "        return_money_magni=return_money/100#配当金を倍率に変換\n",
    "        #state = self.state\n",
    "                                      \n",
    "        #それに応じて購買行動を行う(1～28で購買を行う)\n",
    "        #インデックス番号＋１が購買を行う番号と対応している（はず）\n",
    "        total_use=sum(action)#使った金額の算出\n",
    "        #total_use=sum(action)*self.bet_coefficient#使った金額の算出\n",
    "        \n",
    "        #当たったかどうかの判別と，リターンの算出\n",
    "        race_use=0\n",
    "        for label,pred in zip(np.arange(1, len(action)+1) ,action):#予測のラベル(com)と購買金額の倍率で一緒にfor分を回す\n",
    "            race_use+=pred\n",
    "            if label==result_com:#ラベルと結果が一致したら\n",
    "                race_get=pred*return_money_magni#return_money_magni:配当の倍率(magnification)\n",
    "                #get=pred*self.bet_coefficient*return_money_magni\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #収支計算()\n",
    "        #race_gain=(race_get/race_use)#廃止，これだと０除算になる可能性が高い\n",
    "        race_gain=race_get-race_use#割合でなく，実際の金額で計算．（まあ本番も利益額大きい方がうれしいし？？？）\n",
    "        self.total_use+=race_use\n",
    "        self.total_get+=race_get\n",
    "        if (self.total_get!=0) and (self.total_get!=0):\n",
    "            self.total_gain=((self.total_get/self.total_use)*100)-100#利益率を計算(減ってるとマイナスになる)\n",
    "        else:\n",
    "            self.total_gain=0\n",
    "        before_cash=self.cash#購買行動を行う前の所持金を取っておく\n",
    "        self.cash=self.cash-race_use#使用金額を引く\n",
    "        self.cash=self.cash+race_get#配当金を足す\n",
    "        diff_cash=self.cash-before_cash#購買を行う前と後の金額を比較(減ってるとマイナスになる)\n",
    "        \n",
    "        #報酬の決定\n",
    "        reward=diff_cash#前のレースと比べてお金が増えたか減ったかをそのまま報酬とする\n",
    "#         if diff_cash==0:\n",
    "#             reward=-0.1*bet_coefficient#全く購買を行わないのにも罰則を与える\n",
    "        \n",
    "        #各種フラグの付与（ゲームが続くか続かないか）\n",
    "        if self.cash<self.end_th_cash:#所持金が指定金額以上に減ったらゲームオーバー\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "            \n",
    "        #次の観測（次のレース）の取り出し\n",
    "        self.race_ID=self.race_ID+1#次の状態を抜き出すためにIDに1を足す\n",
    "        if self.race_ID>(len(self.race_df)-1):#次の状態(レース)がなくなったら終了\n",
    "            done=True\n",
    "            self.state=None\n",
    "            reward=None\n",
    "        else:\n",
    "            result_com_next,return_money_next,race_data_next,race_data_label_next,race_date_next=self.split_race_data(self.race_ID)\n",
    "            \n",
    "        #行動分析用の追加評価値\n",
    "        #total_gain=(self.cash/self.st_cash)*100#現時点での利益率\n",
    "        total_income=self.total_get-self.total_use#獲得とそうでないものの差から，今までの利益額を算出\n",
    "        scores_dict={\n",
    "            'total_gain':self.total_gain ,\n",
    "            'total_income':total_income ,\n",
    "            'total_get':self.total_get ,\n",
    "            'total_use':self.total_use\n",
    "        }\n",
    "        return np.array(race_data_next), reward, done, scores_dict\n",
    "\n",
    "    def reset(self):#環境の初期化用メソッド\n",
    "        self.cash = self.st_cash#スタート時の所持金\n",
    "        self.race_ID=0#どこのレースを抜き出すかどうかの番号付け(インデックス参照で抜き出す)\n",
    "        self.total_gain = None#現時点と開始時を比較した時の利益率(reset()で要リセット)\n",
    "        self.total_use = 0#現時点までの総使用金額(reset()で要リセット)\n",
    "        self.total_get = 0#現時点までの総獲得金額(reset()で要リセット)\n",
    "        self.state = None#1レースあたりのデータ格納用のメンバ\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "        #df内の初めのレースの取り出し\n",
    "        result_com,return_money,race_data,race_data_label,race_date=self.split_race_data(self.race_ID)#いろいろ算出用にレースデータの抜き取り\n",
    "        self.state =race_data\n",
    "        \n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def random_action_func(self):#ランダムな行動を生成するための関数\n",
    "        random_action=np.random.rand(28)#サイズ28のランダムな行動を生成する\n",
    "        return random_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-spectacular",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q関数の定義\n",
    "## 入力:学習データそのまま\n",
    "## 出力:1～29(29は購買しない)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "explicit-century",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class QFunction(chainer.Chain):#chainer.Chain:link Lやfunction Fをまとめて管理するものらしい，親クラスとして提供されているためニューラルネットを定義する際はこれを継承させるらしい．\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=100):\n",
    "        super().__init__()\n",
    "        #n_actions:行動の種類数\n",
    "        #obs_size :入力データの次元数\n",
    "        #l1,l2等々は層を表してる，l2が出力層\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)#Lはchainer.linksです（インポート時にas　L　でインポートしてる）\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)#Lはchainer.linksです（インポート時にas　L　でインポートしてる）\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_actions)#Lはchainer.linksです（インポート時にas　L　でインポートしてる）\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (ndarray or chainer.Variable): An observation\n",
    "            test (bool): a flag indicating whether it is in test mode\n",
    "            [翻訳]\n",
    "            x（ndarrayまたはchainer.Variable）：観測値\n",
    "            test（bool）：テストモードかどうかを示すフラグ\n",
    "        \"\"\"\n",
    "        h = F.tanh(self.l0(x))#F は　chainer.functions\n",
    "        h = F.tanh(self.l1(h))#F は　chainer.functions\n",
    "        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-timing",
   "metadata": {},
   "source": [
    "# 試しに動かしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-given",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### envのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "funny-austin",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs [4 46.0 0.45 ... 0 0 0]\n",
      "r -8.0\n",
      "done False\n",
      "info {'total_gain': 0, 'total_income': -8.0, 'total_get': 0.0, 'total_use': 8}\n"
     ]
    }
   ],
   "source": [
    "place_name='asiya'\n",
    "cash=100000#所持金\n",
    "\n",
    "result_filepath=\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "sample_race_df=pd.read_csv(result_filepath)\n",
    "sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "sample_race_df.head()\n",
    "sample_race_df=data_making_mo_bo(sample_race_df)#前処理\n",
    "\n",
    "asiya_env =BoatraceEnv(sample_race_df,cash)#環境の作成（クラス）\n",
    "obs = asiya_env.reset()\n",
    "\n",
    "test_action=[0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0]#たくさん買ってるけどあたってない\n",
    "#test_action=[0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]#一点買いであたった\n",
    "#test_action=[1,2,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]#あたってる\n",
    "obs, r, done, info = asiya_env.step(test_action)\n",
    "print('obs',obs)\n",
    "print('r',r)\n",
    "print('done',done)\n",
    "print('info',info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-duplicate",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "color-clause",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "place_name='asiya'\n",
    "#cash=100000#所持金\n",
    "cash=100#所持金\n",
    "\n",
    "result_filepath=\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "sample_race_df=pd.read_csv(result_filepath)\n",
    "sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "sample_race_df.head()\n",
    "\n",
    "asiya_env =BoatraceEnv(sample_race_df,cash)#環境の作成（クラス）\n",
    "\n",
    "#Q関数の設定\n",
    "obs_size = asiya_env.obs_size\n",
    "n_actions = asiya_env.n_action\n",
    "q_func = QFunction(obs_size, n_actions)\n",
    "#q_func.to_gpu(0) ## GPUを使いたい人はこのコメントを外す\n",
    "\n",
    "#最適化手法とパラメータ設定\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func) #設計したq関数の最適化にAdamを使う\n",
    "gamma = 0.80#報酬の割引率.過去の結果をどのくらい重要視するか\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(#次の戦略を考えるときの方法\n",
    "    epsilon=0.3, random_action_func=asiya_env.random_action_func)\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity = 10**6)#Replayを実行するかどうか\n",
    "phi = lambda x:x.astype(np.float32, copy=False)##型の変換(chainerはfloat32型。float64は駄目)\n",
    "\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "    q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "drawn-system",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "blessed-microwave",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 46.0 0.45 ... 0 0 0]\n",
      "1236\n"
     ]
    }
   ],
   "source": [
    "print(obs)\n",
    "print(len(obs))#入力データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-excitement",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "false-insert",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-455c592bc20f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_episode_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_and_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masiya_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mR\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-ef77cf07efc7>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#それに応じて購買行動を行う(1～28で購買を行う)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m#インデックス番号＋１が購買を行う番号と対応している（はず）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mtotal_use\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#使った金額の算出\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;31m#total_use=sum(action)*self.bet_coefficient#使った金額の算出\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int32' object is not iterable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n_episodes = 200\n",
    "max_episode_len = 200\n",
    "start = time.time()\n",
    "for i in range(1, n_episodes + 1):\n",
    "    v = asiya_env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = asiya_env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'statistics:', agent.get_statistics())\n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "print('Finished, elapsed time : {}'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "novel-daily",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coastal-airline",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3959, 3793, 4196, 3326, 4583, 4037, 4, 1, 35.0, 0.541, 0.14, 2,\n",
       "       1, 40.0, 0.287, 0.19, 2, 1, 30.0, 0.327, 0.19, 3, 1, 45.0, 0.421,\n",
       "       0.16, 2, 1, 23.0, 0.252, 0.18, 3, 1, 34.0, 0.396, 0.19, 3.0, 43.0,\n",
       "       33.0, 40.0, 56.0, 67.0, 59.0, 45.0, 60.0, 42.0, 29.0, 47.0],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-greene",
   "metadata": {},
   "source": [
    "# 会場ごと，comごとにターゲットを指定して，comごとの最適な買い方を学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "romantic-costa",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BoatraceComEnv:#comごとに一つの環境を設定する\n",
    "#class BoatraceEnv(必要であればここに購買金額の係数等を書いてあげる):\n",
    "    def split_race_data(self,race_ID):#レースのID(インデックス)を渡して，レースの情報を学習時などに使いやすい形に切り分けるメソッド\n",
    "        race_row=self.trans_race_df.iloc[race_ID]#対象のレースを切り抜き(series型)\n",
    "        result_com=race_row['result_com']\n",
    "        return_money=race_row['money']\n",
    "        racer_ID=race_row['money']\n",
    "        race_date=race_row['date']\n",
    "        race_data=race_row.drop(['date',\"result_com\",\"money\"]).values\n",
    "        race_data_label=race_row.drop(['date',\"result_com\",\"money\"]).index\n",
    "                                \n",
    "        #race_data:選手情報やボート・モータ番号などの予測用データが入った配列\n",
    "        #race_data_label:予測用データの役割を示す配列(pandasの列名（カラム）がこれに該当する)\n",
    "        return result_com,return_money,race_data,race_data_label,race_date\n",
    "    \n",
    "    def split_race_data_tail(self,race_ID):#レースのID(インデックス)を渡して，レースの情報を学習時などに使いやすい形に切り分けるメソッド,後ろから抽出\n",
    "        race_row=self.trans_race_df.iloc[-1*race_ID]#対象のレースを切り抜き(series型)\n",
    "        result_com=race_row['result_com']\n",
    "        return_money=race_row['money']\n",
    "        racer_ID=race_row['money']\n",
    "        race_date=race_row['date']\n",
    "        race_data=race_row.drop(['date',\"result_com\",\"money\"]).values\n",
    "        race_data_label=race_row.drop(['date',\"result_com\",\"money\"]).index\n",
    "                                \n",
    "        #race_data:選手情報やボート・モータ番号などの予測用データが入った配列\n",
    "        #race_data_label:予測用データの役割を示す配列(pandasの列名（カラム）がこれに該当する)\n",
    "        return result_com,return_money,race_data,race_data_label,race_date\n",
    "    \n",
    "    def trans_target_result_com_df(self):#target_comに応じてresult_comを1,0にラベル変換（df用(全レース)）\n",
    "        transd_race_df=self.race_df.copy()\n",
    "        transd_race_df.loc[transd_race_df['result_com'] != self.target_com, 'result_com'] = 0#target_comと同じでないものは0とする\n",
    "        transd_race_df.loc[transd_race_df['result_com'] == self.target_com, 'result_com'] = 1#target_comと同じものは１とする\n",
    "        \n",
    "        return transd_race_df\n",
    "    \n",
    "    def trans_target_result_com_row(self,race_row):#target_comに応じてresult_comを1,0にラベル変換（行（レース単位）用）多分そんなに使わない，使うとしたら外部で使う\n",
    "        if race_row['result_com']==self.target_com:\n",
    "            race_row['result_com']=1\n",
    "        else:\n",
    "            race_row['result_com']=0\n",
    "        return trans_race_row\n",
    "    \n",
    "    def __init__(self,target_com,race_df,cash,bet_coefficient=10000,end_th_cash_magni=0.5):#メンバ等の定義\n",
    "        self.target_com=target_com#ゲームの対象とするcom\n",
    "        self.st_cash = cash#初めに持っている所持金\n",
    "        self.cash = cash#所持金（初めは上のものと同じ値が入るが，こちらの変数はゲームが進んでいくと更新されていく）\n",
    "        self.end_th_cash = cash*end_th_cash_magni#所持金がいくらを下回ったらゲームオーバーか決めるための閾値金額(end_th_cashで何パーセントを下回ったらアウトかを指定)\n",
    "        self.bet_coefficient = bet_coefficient#購買時を想定して定数倍する係数\n",
    "        self.random_state = 7#何かと生成をするときに使うためのシード値\n",
    "        self.race_ID=0#どこのレースを抜き出すかどうかの番号付け(インデックス参照で抜き出す)\n",
    "        self.total_gain = None#現時点と開始時を比較した時の利益率(reset()で要リセット)\n",
    "        self.total_use = 0#現時点までの総使用金額(reset()で要リセット)\n",
    "        self.total_get = 0#現時点までの総獲得金額(reset()で要リセット)\n",
    "        self.num_hit = 0#現時点までの総的中回数(reset()で要リセット)\n",
    "        self.num_pass = 0#現時点までの購買を行わなかった数(reset()で要リセット)\n",
    "        self.num_bet = 0#現時点までの購買行動を行った数(reset()で要リセット)\n",
    "        self.total_reward=0#報酬の合計（報酬設計の参考にする）(reset()で要リセット)\n",
    "        self.state = None#1レースあたりのデータ格納用のメンバ\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "        #ラベル（result_com）をtarget_comをもとに変換する\n",
    "        self.race_df=race_df\n",
    "        trans_race_df=self.trans_target_result_com_df()\n",
    "        self.trans_race_df=trans_race_df#全部のレース情報のまとめdfを格納\n",
    "        \n",
    "        #Q関数定義時関連の処理\n",
    "        self.n_action=11#行動の種類(買うべき指数的な感じ（単純に倍率とするか，指数とするかは決めていない）０は購買しない，それ以降は購買で数値が上がっていくほど多くの購買を行う)\n",
    "        sample_df=self.trans_race_df\n",
    "        sample_df=sample_df.drop(['date',\"result_com\",\"money\"],axis=1)\n",
    "        self.obs_size=len(sample_df.columns)#予測に使う情報の次元数\n",
    "\n",
    "#     def seed(self, seed=None):\n",
    "#         self.np_random, seed = seeding.np_random(seed)\n",
    "#         return [seed]\n",
    "\n",
    "\n",
    "    def step(self, action):#ステップ（ゲームの報酬計算と次の環境の取得）\n",
    "        race_ID=self.race_ID\n",
    "        #result_com,return_money,race_data,race_data_label,race_date=self.split_race_data(self.race_ID)#いろいろ算出用にレースデータの抜き取り\n",
    "        result_com,return_money,race_data,race_data_label,race_date=self.split_race_data_tail(self.race_ID)#いろいろ算出用にレースデータの抜き取り\n",
    "        return_money_magni=return_money/100#配当金を倍率に変換\n",
    "        #state = self.state\n",
    "        #当たったかどうかの判別と，リターンの算出\n",
    "        #race_use=0\n",
    "        race_get=0\n",
    "        race_use=action\n",
    "        #race_use+=action*self.bet_coefficient\n",
    "        if (action!=0):\n",
    "            self.num_bet=self.num_bet+1\n",
    "            if result_com==1:#購買行動を行っていたかつ，的中した         \n",
    "                race_get=action*return_money_magni#return_money_magni:配当の倍率(magnification)\n",
    "                self.num_hit=self.num_hit+1\n",
    "                #race_get=self.bet_coefficient*action*return_money_magni#実際の購買額版，return_money_magni:配当の倍率(magnification)\n",
    "        elif(action==0):#購買を行わなかった（レースを見逃した）\n",
    "            self.num_pass =self.num_pass+1 \n",
    "        else:\n",
    "            print('error_unexpected_join_else::この警告の表示は想定されてません')\n",
    "        \n",
    "        #収支計算()\n",
    "        #race_gain=(race_get/race_use)#廃止，これだと０除算になる可能性が高い\n",
    "        race_gain=race_get-race_use#割合でなく，実際の金額で計算．（まあ本番も利益額大きい方がうれしいし？？？）\n",
    "        self.total_use+=race_use\n",
    "        self.total_get+=race_get\n",
    "        if (self.total_get!=0) and (self.total_get!=0):\n",
    "            self.total_gain=((self.total_get/self.total_use)*100)-100#利益率を計算(減ってるとマイナスになる)\n",
    "        else:\n",
    "            self.total_gain=0\n",
    "        before_cash=self.cash#購買行動を行う前の所持金を取っておく\n",
    "        self.cash=self.cash-race_use#使用金額を引く\n",
    "        self.cash=self.cash+race_get#配当金を足す\n",
    "        #diff_cash=self.cash-before_cash#購買を行う前と後の金額を比較(減ってるとマイナスになる)\n",
    "        #出現したが，見逃したことへの罰則を与える（全く買わない事が最適解と認識するのをさける）\n",
    "        if (result_com==1) and (action==0):\n",
    "            diff_cash=(self.cash-before_cash)*1.1#購買を行う前と後の金額を比較,また，出現しているのにも関わらず見逃したので罰則を与える(減ってるとマイナスになる)\n",
    "        else:\n",
    "            diff_cash=self.cash-before_cash#購買を行う前と後の金額を比較(減ってるとマイナスになる)\n",
    "        \n",
    "        #報酬の決定\n",
    "        reward=diff_cash#前のレースと比べてお金が増えたか減ったかをそのまま報酬とする\n",
    "        if reward<-100:\n",
    "            print('--------------------------------------------------------------------\\n')\n",
    "            print('=======================diff_cash:{}======================='.format(diff_cash))\n",
    "            print('=======================cash:{}======================='.format(self.cash))\n",
    "            print('=======================before_cash:{}======================='.format(before_cash))\n",
    "            print('=======================race_use:{}======================='.format(race_use))\n",
    "            print('=======================race_get:{}======================='.format(race_get))\n",
    "            print('=======================action:{}======================='.format(action))\n",
    "            print('--------------------------------------------------------------------\\n')\n",
    "#         if (diff_cash==0)and(result_com==1):\n",
    "#             reward=-15#出現しているが，見逃したものには罰則を与える\n",
    "        self.total_reward=self.total_reward+reward\n",
    "        #各種フラグの付与（ゲームが続くか続かないか）\n",
    "        if self.cash<self.end_th_cash:#所持金が指定金額以上に減ったらゲームオーバー\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "            \n",
    "        #次の観測（次のレース）の取り出し\n",
    "        self.race_ID=self.race_ID+1#次の状態を抜き出すためにIDに1を足す\n",
    "        if self.race_ID>(len(self.race_df)-1):#次の状態(レース)がなくなったら終了\n",
    "            done=True\n",
    "            self.state=None\n",
    "            reward=None\n",
    "        else:\n",
    "            result_com_next,return_money_next,race_data_next,race_data_label_next,race_date_next=self.split_race_data(self.race_ID)\n",
    "            \n",
    "        #行動分析用の追加評価値\n",
    "        #total_gain=(self.cash/self.st_cash)*100#現時点での利益率\n",
    "        total_income=self.total_get-self.total_use#獲得とそうでないものの差から，今までの利益額を算出\n",
    "        try:\n",
    "            buy_hit_per=(self.num_hit/self.num_bet)*100\n",
    "        except:\n",
    "            buy_hit_per=0\n",
    "        scores_dict={\n",
    "            'total_gain':self.total_gain ,\n",
    "            'total_income':total_income ,\n",
    "            'total_get':self.total_get ,\n",
    "            'total_use':self.total_use,\n",
    "            'num_bet':self.num_bet,\n",
    "            'num_pass':self.num_pass,\n",
    "            'num_hit':self.num_hit,\n",
    "            'buy_hit_per':buy_hit_per,\n",
    "            'total_reward':self.total_reward\n",
    "        }\n",
    "        \n",
    "        return np.array(race_data_next), reward, done, scores_dict\n",
    "\n",
    "    def reset(self):#環境の初期化用メソッド\n",
    "        self.cash = self.st_cash#スタート時の所持金\n",
    "        self.race_ID=0#どこのレースを抜き出すかどうかの番号付け(インデックス参照で抜き出す)\n",
    "        self.total_gain = None#現時点と開始時を比較した時の利益率(reset()で要リセット)\n",
    "        self.total_use = 0#現時点までの総使用金額(reset()で要リセット)\n",
    "        self.total_get = 0#現時点までの総獲得金額(reset()で要リセット)\n",
    "        self.num_hit = 0#現時点までの総的中回数(reset()で要リセット)\n",
    "        self.num_pass = 0#現時点までの購買を行わなかった数(reset()で要リセット)\n",
    "        self.num_bet = 0#現時点までの購買行動を行った数(reset()で要リセット)\n",
    "        self.total_reward=0#報酬の合計（報酬設計の参考にする）\n",
    "        self.state = None#1レースあたりのデータ格納用のメンバ\n",
    "        self.steps_beyond_done = None\n",
    "        \n",
    "        #df内の初めのレースの取り出し\n",
    "        result_com,return_money,race_data,race_data_label,race_date=self.split_race_data(self.race_ID)#いろいろ算出用にレースデータの抜き取り\n",
    "        self.state =race_data\n",
    "        \n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def random_action_func(self):#ランダムな行動を生成するための関数\n",
    "        random_action=np.random.randint(11)#サイズ0～10のランダムな行動を生成する\n",
    "        return random_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-neighborhood",
   "metadata": {},
   "source": [
    "# Q関数の定義\n",
    "## 入力:学習データそのまま\n",
    "## 出力:0～11(0は購買しない)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vietnamese-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QFunction(chainer.Chain):#chainer.Chain:link Lやfunction Fをまとめて管理するものらしい，親クラスとして提供されているためニューラルネットを定義する際はこれを継承させるらしい．\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=500):\n",
    "        super().__init__()\n",
    "        #n_actions:行動の種類数\n",
    "        #obs_size :入力データの次元数\n",
    "        #l1,l2等々は層を表してる，l2が出力層\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)#Lはchainer.linksです（インポート時にas　L　でインポートしてる）\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)#Lはchainer.linksです（インポート時にas　L　でインポートしてる）\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_actions)#Lはchainer.linksです（インポート時にas　L　でインポートしてる）\n",
    "            #self.l3 = L.Linear(n_hidden_channels, n_actions)#Lはchainer.linksです（インポート時にas　L　でインポートしてる）\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (ndarray or chainer.Variable): An observation\n",
    "            test (bool): a flag indicating whether it is in test mode\n",
    "            [翻訳]\n",
    "            x（ndarrayまたはchainer.Variable）：観測値\n",
    "            test（bool）：テストモードかどうかを示すフラグ\n",
    "        \"\"\"\n",
    "        h = F.tanh(self.l0(x))#F は　chainer.functions\n",
    "        h = F.tanh(self.l1(h))#F は　chainer.functions\n",
    "        #h = F.tanh(self.l2(h))#F は　chainer.functions\n",
    "        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-cemetery",
   "metadata": {},
   "source": [
    "## テスト動作\n",
    "terget_comを一つだけにして挙動だけ確認する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-prevention",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### envのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "clear-aquarium",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../boatracer_BOT_making/bot_database/asiya/asiya_train/train_asiya.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-50a50e4e845a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mresult_filepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplace_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplace_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#作成したデータの書き込み先#使用するデータの読み込み\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msample_race_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0msample_race_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_race_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Unnamed: 0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msample_race_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../boatracer_BOT_making/bot_database/asiya/asiya_train/train_asiya.csv'"
     ]
    }
   ],
   "source": [
    "# place_name='asiya'\n",
    "# cash=100000#所持金\n",
    "# target_com=6\n",
    "# #target_com=1\n",
    "\n",
    "\n",
    "# result_filepath=\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "# sample_race_df=pd.read_csv(result_filepath)\n",
    "# sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "# sample_race_df.head()\n",
    "# sample_race_df=data_making_mo_bo(sample_race_df)#前処理\n",
    "\n",
    "# asiya_env =BoatraceComEnv(target_com,sample_race_df,cash)#環境の作成（クラス）\n",
    "# obs = asiya_env.reset()\n",
    "\n",
    "# #test_action=0#買わない\n",
    "# #test_action=1#一点買いであたった\n",
    "# test_action=5#あたってる\n",
    "# obs, r, done, info = asiya_env.step(test_action)\n",
    "# print('obs',obs)\n",
    "# print('r',r)\n",
    "# print('done',done)\n",
    "# print('info',info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-fusion",
   "metadata": {},
   "source": [
    "### 定義テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "posted-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_name='asiya'\n",
    "#cash=100000#所持金\n",
    "cash=10000#所持金\n",
    "target_com=5\n",
    "#target_com=2\n",
    "\n",
    "result_filepath=\"../../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "sample_race_df=pd.read_csv(result_filepath)\n",
    "sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "sample_race_df.head()\n",
    "sample_race_df=data_making(sample_race_df)#前処理\n",
    "#sample_race_df[(sample_race_df['date'].dt.year>2019)&(sample_race_df['date'].dt.year<=2021)]\n",
    "sample_race_df=sample_race_df[sample_race_df['date'].dt.year<=2020]#検証用データに答えが漏れないように切り抜く\n",
    "asiya_env =BoatraceComEnv(target_com,sample_race_df,cash)#環境の作成（クラス）\n",
    "\n",
    "#Q関数の設定\n",
    "obs_size = asiya_env.obs_size\n",
    "n_actions = asiya_env.n_action\n",
    "q_func = QFunction(obs_size, n_actions)\n",
    "q_func.to_gpu(0)\n",
    "#q_func.to_gpu(0) ## GPUを使いたい人はこのコメントを外す\n",
    "\n",
    "#最適化手法とパラメータ設定\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func) #設計したq関数の最適化にAdamを使う\n",
    "gamma = 0.5#報酬の割引率.過去の結果をどのくらい重要視するか\n",
    "#gamma = 0.06#報酬の割引率.過去の結果をどのくらい重要視するか\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(#次の戦略を考えるときの方法\n",
    "    epsilon=0.5, random_action_func=asiya_env.random_action_func)\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity = 10**6)#Replayを実行するかどうか\n",
    "phi = lambda x:x.astype(np.float32, copy=False)##型の変換(chainerはfloat32型。float64は駄目)\n",
    "\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "    q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-funds",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "religious-attention",
   "metadata": {},
   "source": [
    "### 学習テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "focused-oracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10 R: -267.19999999999345 statistics: [('average_q', 0.21124247384792283), ('average_loss', 1.9603527536517853), ('n_updates', 22663)]\n",
      "episode: 20 R: -300.29999999999563 statistics: [('average_q', 0.5839746339527728), ('average_loss', 2.223724643373871), ('n_updates', 42663)]\n",
      "episode: 30 R: -508.69999999999527 statistics: [('average_q', 0.866187330443824), ('average_loss', 2.200110057483495), ('n_updates', 62663)]\n",
      "episode: 40 R: -305.0000000000073 statistics: [('average_q', 1.180387288046828), ('average_loss', 2.278294312183965), ('n_updates', 82663)]\n",
      "episode: 50 R: -510.00000000000364 statistics: [('average_q', 1.2353785781663726), ('average_loss', 2.905858145388548), ('n_updates', 102663)]\n",
      "episode: 60 R: 416.2000000000007 statistics: [('average_q', 1.351919822240334), ('average_loss', 2.381200705327402), ('n_updates', 122663)]\n",
      "episode: 70 R: -188.20000000000073 statistics: [('average_q', 1.3162335493596238), ('average_loss', 2.5636812895072625), ('n_updates', 142663)]\n",
      "episode: 80 R: 1092.800000000003 statistics: [('average_q', 1.4258748151651242), ('average_loss', 2.310280997836283), ('n_updates', 162663)]\n",
      "episode: 90 R: 860.2000000000007 statistics: [('average_q', 1.5031333205539485), ('average_loss', 2.311103421309353), ('n_updates', 182663)]\n",
      "episode: 100 R: 712.3000000000029 statistics: [('average_q', 1.5230338194673865), ('average_loss', 2.4133785740757805), ('n_updates', 202663)]\n",
      "episode: 110 R: 219.1999999999971 statistics: [('average_q', 1.6109323551926404), ('average_loss', 2.6038281420550318), ('n_updates', 222663)]\n",
      "episode: 120 R: 1120.0999999999967 statistics: [('average_q', 1.6875066654540483), ('average_loss', 2.4688554177906763), ('n_updates', 242663)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DoubleDQN' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a3104259c5dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mepisode_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total_gain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total_gain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mmax_agent_gain\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m:\u001b[0m\u001b[1;31m#良いエージェントかつ記録を更新するたびに保存を行う\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0msave_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0msave_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_cpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0msave_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../agent/gamma05_agent/gamma_05_gain{}_agent'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total_gain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DoubleDQN' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "n_episodes = 5000\n",
    "max_episode_len = 2000\n",
    "start = time.time()\n",
    "episode_step=list()\n",
    "max_agent_gain=0\n",
    "scores_df=pd.DataFrame(columns=['total_gain','total_income','total_get','total_use','num_bet','num_pass','num_hit','buy_hit_per'])\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = asiya_env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, option = asiya_env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'statistics:', agent.get_statistics())\n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    add_df = pd.DataFrame.from_dict(option, orient='index').T\n",
    "    scores_df = pd.concat([scores_df, add_df], axis=0)\n",
    "    episode_step.append(i)\n",
    "    if (option['total_gain']>40) and (option['total_gain']>max_agent_gain)  :#良いエージェントかつ記録を更新するたびに保存を行う\n",
    "        save_agent=agent.copy()\n",
    "        save_agent.to_cpu()\n",
    "        save_agent.save('../agent/gamma05_agent/gamma_05_gain{}_agent'.format(int(option['total_gain'])))\n",
    "        #save_agent.save('../agent/gamma05_agent/CPU_gamma_05_gain{}_agent'.format(int(option['total_gain'])))\n",
    "\n",
    "        max_agent_gain=option['total_gain']\n",
    "                   \n",
    "    #エピソードごとの成績を作成\n",
    "scores_df['n_episode']=episode_step\n",
    "print('Finished, elapsed time : {}'.format(time.time()-start))\n",
    "\n",
    "scores_df.to_csv('csv/gamma_05_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "humanitarian-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.save('../agent/gamma_05_shippai_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-resource",
   "metadata": {},
   "source": [
    "### テスト　学習過程の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-equality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def larning_plt(df,x_n,y_n):#2本軸でのグラフの描写\n",
    "    fig = plt.figure(figsize=(19,8))\n",
    "    plt.tick_params(colors='black')\n",
    "    plt.rcParams[\"font.size\"] = 18\n",
    "    fig.set_facecolor(color='white')\n",
    "\n",
    "    x=df[x_n].values\n",
    "    y=df[y_n].values\n",
    "\n",
    "    ax1 = fig.add_subplot()\n",
    "    #ax1.plot(x, y,label=y_n, marker=\"o\")\n",
    "    ax1.plot(x, y,label=y_n)\n",
    "    ax1.tick_params()\n",
    "    \n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    ax1.legend(h1, l1)\n",
    "\n",
    "    ax1.set_xlabel(x_n)\n",
    "    ax1.set_ylabel(y_n)\n",
    "    plt.axhline(y=0,color='red')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "for col in scores_df.columns:\n",
    "    x_n='n_episode'\n",
    "    y_n=col\n",
    "    larning_plt(scores_df,x_n,y_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "major-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv('csv/20220803.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-thong",
   "metadata": {},
   "source": [
    "## GPUの使用でどれくらい短縮効果があるかの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-miracle",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### GPUなし:911sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conservative-jacket",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10 R: -269454.1750000002 statistics: [('average_q', -0.08670020672760897), ('average_loss', 112.53643541119972), ('n_updates', 19500)]\n",
      "episode: 20 R: -231561.5200000003 statistics: [('average_q', -0.175727651092178), ('average_loss', 133.41969647892438), ('n_updates', 39500)]\n",
      "episode: 30 R: -254537.3250000003 statistics: [('average_q', -0.12066549715352473), ('average_loss', 135.91506888733377), ('n_updates', 59500)]\n",
      "episode: 40 R: -285264.0250000004 statistics: [('average_q', 0.025761359124444258), ('average_loss', 116.76136684894085), ('n_updates', 79500)]\n",
      "episode: 50 R: -294924.93500000035 statistics: [('average_q', 0.006402642496736394), ('average_loss', 125.70122892190575), ('n_updates', 99500)]\n",
      "Finished, elapsed time : 911.2012474536896\n"
     ]
    }
   ],
   "source": [
    "place_name='asiya'\n",
    "#cash=100000#所持金\n",
    "cash=100000#所持金\n",
    "target_com=5\n",
    "#target_com=2\n",
    "\n",
    "result_filepath=\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "sample_race_df=pd.read_csv(result_filepath)\n",
    "sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "sample_race_df.head()\n",
    "sample_race_df=data_making(sample_race_df)#前処理\n",
    "asiya_env =BoatraceComEnv(target_com,sample_race_df,cash)#環境の作成（クラス）\n",
    "\n",
    "#Q関数の設定\n",
    "obs_size = asiya_env.obs_size\n",
    "n_actions = asiya_env.n_action\n",
    "q_func = QFunction(obs_size, n_actions)\n",
    "#q_func.to_gpu(0) ## GPUを使いたい人はこのコメントを外す\n",
    "\n",
    "#最適化手法とパラメータ設定\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func) #設計したq関数の最適化にAdamを使う\n",
    "gamma = 0.80#報酬の割引率.過去の結果をどのくらい重要視するか\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(#次の戦略を考えるときの方法\n",
    "    epsilon=0.3, random_action_func=asiya_env.random_action_func)\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity = 10**6)#Replayを実行するかどうか\n",
    "phi = lambda x:x.astype(np.float32, copy=False)##型の変換(chainerはfloat32型。float64は駄目)\n",
    "\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "    q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)\n",
    "\n",
    "import time\n",
    "n_episodes = 50\n",
    "max_episode_len = 2000\n",
    "start = time.time()\n",
    "episode_step=list()\n",
    "episode_gain=list()\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = asiya_env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, option = asiya_env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'statistics:', agent.get_statistics())\n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    episode_step.append(i)\n",
    "    episode_gain.append(asiya_env.total_gain)\n",
    "    #エピソードごとの成績を作成\n",
    "    \n",
    "print('Finished, elapsed time : {}'.format(time.time()-start))\n",
    "analysis_df=pd.DataFrame()\n",
    "analysis_df['ep_step']=episode_step\n",
    "analysis_df['ep_gain']=episode_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-favorite",
   "metadata": {},
   "source": [
    "### GPUあり:665sec==早い！！,今度っからこっちでやる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_name='asiya'\n",
    "#cash=100000#所持金\n",
    "cash=100000#所持金\n",
    "target_com=5\n",
    "#target_com=2\n",
    "\n",
    "result_filepath=\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "sample_race_df=pd.read_csv(result_filepath)\n",
    "sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "sample_race_df.head()\n",
    "sample_race_df=data_making(sample_race_df)#前処理\n",
    "asiya_env =BoatraceComEnv(target_com,sample_race_df,cash)#環境の作成（クラス）\n",
    "\n",
    "#Q関数の設定\n",
    "obs_size = asiya_env.obs_size\n",
    "n_actions = asiya_env.n_action\n",
    "q_func = QFunction(obs_size, n_actions)\n",
    "q_func.to_gpu(0)\n",
    "#q_func.to_gpu(0) ## GPUを使いたい人はこのコメントを外す\n",
    "\n",
    "#最適化手法とパラメータ設定\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func) #設計したq関数の最適化にAdamを使う\n",
    "gamma = 0.80#報酬の割引率.過去の結果をどのくらい重要視するか\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(#次の戦略を考えるときの方法\n",
    "    epsilon=0.3, random_action_func=asiya_env.random_action_func)\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity = 10**6)#Replayを実行するかどうか\n",
    "phi = lambda x:x.astype(np.float32, copy=False)##型の変換(chainerはfloat32型。float64は駄目)\n",
    "\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "    q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "n_episodes = 50\n",
    "max_episode_len = 2000\n",
    "start = time.time()\n",
    "episode_step=list()\n",
    "episode_gain=list()\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = asiya_env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, option = asiya_env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'statistics:', agent.get_statistics())\n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    episode_step.append(i)\n",
    "    episode_gain.append(asiya_env.total_gain)\n",
    "    #エピソードごとの成績を作成\n",
    "    \n",
    "print('Finished, elapsed time : {}'.format(time.time()-start))\n",
    "analysis_df=pd.DataFrame()\n",
    "analysis_df['ep_step']=episode_step\n",
    "analysis_df['ep_gain']=episode_gain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-warrior",
   "metadata": {},
   "source": [
    "## 成長過程をもっと可視化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_name='asiya'\n",
    "#cash=100000#所持金\n",
    "cash=100000#所持金\n",
    "target_com=5\n",
    "#target_com=2\n",
    "\n",
    "result_filepath=\"../boatracer_BOT_making/bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "sample_race_df=pd.read_csv(result_filepath)\n",
    "sample_race_df=sample_race_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "sample_race_df.head()\n",
    "sample_race_df=data_making(sample_race_df)#前処理\n",
    "asiya_env =BoatraceComEnv(target_com,sample_race_df,cash)#環境の作成（クラス）\n",
    "\n",
    "#Q関数の設定\n",
    "obs_size = asiya_env.obs_size\n",
    "n_actions = asiya_env.n_action\n",
    "q_func = QFunction(obs_size, n_actions)\n",
    "q_func.to_gpu(0)\n",
    "#q_func.to_gpu(0) ## GPUを使いたい人はこのコメントを外す\n",
    "\n",
    "#最適化手法とパラメータ設定\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func) #設計したq関数の最適化にAdamを使う\n",
    "gamma = 0.80#報酬の割引率.過去の結果をどのくらい重要視するか\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(#次の戦略を考えるときの方法\n",
    "    epsilon=0.3, random_action_func=asiya_env.random_action_func)\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity = 10**6)#Replayを実行するかどうか\n",
    "phi = lambda x:x.astype(np.float32, copy=False)##型の変換(chainerはfloat32型。float64は駄目)\n",
    "\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "    q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "n_episodes = 50\n",
    "max_episode_len = 2000\n",
    "start = time.time()\n",
    "episode_step=list()\n",
    "episode_gain=list()\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = asiya_env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, option = asiya_env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'statistics:', agent.get_statistics())\n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "    episode_step.append(i)\n",
    "    episode_gain.append(asiya_env.total_gain)\n",
    "    #エピソードごとの成績を作成\n",
    "    \n",
    "print('Finished, elapsed time : {}'.format(time.time()-start))\n",
    "analysis_df=pd.DataFrame()\n",
    "analysis_df['ep_step']=episode_step\n",
    "analysis_df['ep_gain']=episode_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elementary-detroit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ep_step</th>\n",
       "      <th>ep_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-25.809487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-66.960916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-40.341060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.718844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-51.125041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-2.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-33.653782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-23.351869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-12.297432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-23.516829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-48.226568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>19.580897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>-46.262238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>-30.707333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.150962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>-33.893315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>-50.364360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>-28.741787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>4.838070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-30.381398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>-26.877843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>-10.665033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>-36.925278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>-8.134052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>-20.513912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>-43.180534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-28.180898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>-23.713995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>9.422680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>-36.579194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>-44.033758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>-46.088136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>-24.976974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>-24.273340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>-26.117144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>-57.910891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>-54.102564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>-20.991649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>-58.981002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>-43.576818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>-31.707481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>-58.399448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>-35.848765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>-45.568641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>-18.878893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>-53.970297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>-30.497857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>-11.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>-54.153024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>6.269103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ep_step    ep_gain\n",
       "0         1 -25.809487\n",
       "1         2 -66.960916\n",
       "2         3 -40.341060\n",
       "3         4  -0.718844\n",
       "4         5 -51.125041\n",
       "5         6  -2.692308\n",
       "6         7 -33.653782\n",
       "7         8 -23.351869\n",
       "8         9 -12.297432\n",
       "9        10 -23.516829\n",
       "10       11 -48.226568\n",
       "11       12  19.580897\n",
       "12       13 -46.262238\n",
       "13       14 -30.707333\n",
       "14       15   1.150962\n",
       "15       16 -33.893315\n",
       "16       17 -50.364360\n",
       "17       18 -28.741787\n",
       "18       19   4.838070\n",
       "19       20 -30.381398\n",
       "20       21 -26.877843\n",
       "21       22 -10.665033\n",
       "22       23 -36.925278\n",
       "23       24  -8.134052\n",
       "24       25 -20.513912\n",
       "25       26 -43.180534\n",
       "26       27 -28.180898\n",
       "27       28 -23.713995\n",
       "28       29   9.422680\n",
       "29       30 -36.579194\n",
       "30       31 -44.033758\n",
       "31       32 -46.088136\n",
       "32       33 -24.976974\n",
       "33       34 -24.273340\n",
       "34       35 -26.117144\n",
       "35       36 -57.910891\n",
       "36       37 -54.102564\n",
       "37       38 -20.991649\n",
       "38       39 -58.981002\n",
       "39       40 -43.576818\n",
       "40       41 -31.707481\n",
       "41       42 -58.399448\n",
       "42       43 -35.848765\n",
       "43       44 -45.568641\n",
       "44       45 -18.878893\n",
       "45       46 -53.970297\n",
       "46       47 -30.497857\n",
       "47       48 -11.822581\n",
       "48       49 -54.153024\n",
       "49       50   6.269103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-pledge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-footage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coral-medicare",
   "metadata": {},
   "source": [
    "# 参考リンク集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-transcription",
   "metadata": {},
   "source": [
    "・強化学習でビットコイントレードしたいかもしれないので役に立ちそうなリンクをメモしとく<br>\n",
    "https://scrapbox.io/arms22/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%81%A7%E3%83%93%E3%83%83%E3%83%88%E3%82%B3%E3%82%A4%E3%83%B3%E3%83%88%E3%83%AC%E3%83%BC%E3%83%89%E3%81%97%E3%81%9F%E3%81%84%E3%81%8B%E3%82%82%E3%81%97%E3%82%8C%E3%81%AA%E3%81%84%E3%81%AE%E3%81%A7%E5%BD%B9%E3%81%AB%E7%AB%8B%E3%81%A1%E3%81%9D%E3%81%86%E3%81%AA%E3%83%AA%E3%83%B3%E3%82%AF%E3%82%92%E3%83%A1%E3%83%A2%E3%81%97%E3%81%A8%E3%81%8F<br>\n",
    "イラスト多めで概念の理解に助かる : https://meet.metaps.com/entry/2017/08/10/ai/<br>\n",
    "トレード，成功例  :  https://qiita.com/ryo_grid/items/ae78835f9a61d020145f<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-wallet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
